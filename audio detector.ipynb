{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\n\nimport os\nimport sys\nfrom tempfile import NamedTemporaryFile\nfrom urllib.request import urlopen\nfrom urllib.parse import unquote, urlparse\nfrom urllib.error import HTTPError\nfrom zipfile import ZipFile\nimport tarfile\nimport shutil\n\nCHUNK_SIZE = 40960\nDATA_SOURCE_MAPPING = 'ravdess-emotional-speech-audio:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F107620%2F256618%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240412%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240412T050045Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1e1d4efe58dfcc7be4f7df6d04df610a06543ccd4ebfee560e6034c188c837947c47c8b287259474a88ed2805ac952252e3491cc484b29c8f1e6191b985caf6a0503ae47a981cb2542bd1df50cd0ff84a6c93c7cbf43ee207bc844f3fdd142cc658aae0f23fd2fcfe4ee76333ddb3c8b8459f0b750840ca8af244d480f1c36e97a84c2414a51a29f8ae49213c72c7c00b94316aec7e682418d507bd8a723bc883f2848e24a0d73b7a5c350a9b581667d0a63ee0e1490041df3482a65c1a94361a715fb5c326f49fe8296030de39218eaa91201c66bcd26973226dbdc726c3eed9bf872ec9f79d32edfaa38a5a49ffc0277837c3ae378f6a2dd9d9927b3dd8775'\n\nKAGGLE_INPUT_PATH='/kaggle/input'\nKAGGLE_WORKING_PATH='/kaggle/working'\nKAGGLE_SYMLINK='kaggle'\n\n!umount /kaggle/input/ 2> /dev/null\nshutil.rmtree('/kaggle/input', ignore_errors=True)\nos.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\nos.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n\ntry:\n  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\nexcept FileExistsError:\n  pass\ntry:\n  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\nexcept FileExistsError:\n  pass\n\nfor data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n    directory, download_url_encoded = data_source_mapping.split(':')\n    download_url = unquote(download_url_encoded)\n    filename = urlparse(download_url).path\n    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n    try:\n        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n            total_length = fileres.headers['content-length']\n            print(f'Downloading {directory}, {total_length} bytes compressed')\n            dl = 0\n            data = fileres.read(CHUNK_SIZE)\n            while len(data) > 0:\n                dl += len(data)\n                tfile.write(data)\n                done = int(50 * dl / int(total_length))\n                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n                sys.stdout.flush()\n                data = fileres.read(CHUNK_SIZE)\n            if filename.endswith('.zip'):\n              with ZipFile(tfile) as zfile:\n                zfile.extractall(destination_path)\n            else:\n              with tarfile.open(tfile.name) as tarfile:\n                tarfile.extractall(destination_path)\n            print(f'\\nDownloaded and uncompressed: {directory}')\n    except HTTPError as e:\n        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n        continue\n    except OSError as e:\n        print(f'Failed to load {download_url} to path {destination_path}')\n        continue\n\nprint('Data source import complete.')\n","metadata":{"id":"duXKkYTxrfv8","outputId":"068270b7-aa31-4733-f625-e55fd6fc9830","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sounddevice\n","metadata":{"id":"o6KkOeUXwpXY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install python_speech_features","metadata":{"id":"pfOf5WlIrfv_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install SpeechRecognition","metadata":{"id":"6uw7ISlzsvPy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install noisereduce","metadata":{"id":"szG76wChr_Ba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install soundfile","metadata":{"id":"mwJP2ol5y_2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport glob\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.io import wavfile\nfrom python_speech_features import mfcc , logfbank\nimport librosa as lr\nimport os, glob, pickle\nimport librosa\nfrom scipy import signal\nimport noisereduce as nr\nfrom glob import glob\nimport librosa\nget_ipython().magic('matplotlib inline')\n\nimport soundfile\nfrom tensorflow.keras.layers import Conv2D,MaxPool2D, Flatten, LSTM\nfrom keras.layers import Dropout,Dense,TimeDistributed\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score","metadata":{"id":"MLGlFWWFrfv_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading the required RAVDESS DataSet\n\nos.listdir(path='/kaggle/input/ravdess-emotional-speech-audio')\ndef getListOfFiles(dirName):\n    listOfFile=os.listdir(dirName)\n    allFiles=list()\n    for entry in listOfFile:\n        fullPath=os.path.join(dirName, entry)\n        if os.path.isdir(fullPath):\n            allFiles=allFiles + getListOfFiles(fullPath)\n        else:\n            allFiles.append(fullPath)\n    return allFiles\n\ndirName = '/kaggle/input/ravdess-emotional-speech-audio'\nlistOfFiles = getListOfFiles(dirName)\nlen(listOfFiles)","metadata":{"id":"8lgqalOorfwA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c28ae2ac-d415-4ea1-e8d8-4057811df859"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Use the Speech-Recognition API to get the Raw Text from Audio Files, using Google Speech Recognition\n\nimport speech_recognition as sr\n\nr = sr.Recognizer()\n\ncounter = 0  # Counter variable to keep track of the number of files processed\n\nfor file in range(0, min(100, len(listOfFiles)), 1):  # Loop through the first 100 files or less\n    with sr.AudioFile(listOfFiles[file]) as source:\n        audio = r.listen(source)\n        try:\n            text = r.recognize_google(audio)\n            print(text)\n        except sr.UnknownValueError:\n            print(\"Speech Recognition could not understand audio\")\n        except sr.RequestError as e:\n            print(\"Could not request results from Speech Recognition service; {0}\".format(e))\n        counter += 1\n        if counter >= 100:\n            break\n","metadata":{"id":"fBiZtd0NrfwA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f28ad6af-4a6c-472b-e6e3-c17f01739b8a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Down sampling audio files and putting them into a folder (clean_speech_audio)\n\ndef envelope(y , rate, threshold):\n    mask=[]\n    y=pd.Series(y).apply(np.abs)\n    y_mean = y.rolling(window=int(rate/10) ,  min_periods=1 , center = True).mean()\n    for mean in y_mean:\n        if mean>threshold:\n            mask.append(True)\n        else:\n            mask.append(False)\n    return mask","metadata":{"id":"gfsJ38SLrfwA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting the Basic Graphs for understanding of Audio Files and Spectogram\nfor file in range(0 , len(listOfFiles) , 1):\n    audio , sfreq = lr.load(listOfFiles[file])\n    time = np.arange(0 , len(audio)) / sfreq\n\n    fig ,ax = plt.subplots()\n    ax.plot(time , audio)\n    ax.set(xlabel = 'Time (s)' , ylabel = 'Sound Amplitude')\n    plt.show()\n\n#PLOT THE SEPCTOGRAM\nfor file in range(0 , len(listOfFiles) , 1):\n     sample_rate , samples = wavfile.read(listOfFiles[file])\n     frequencies , times, spectrogram = signal.spectrogram(samples, sample_rate)\n     plt.pcolormesh(times, frequencies, spectrogram)\n     plt.imshow(spectrogram)\n     plt.ylabel('Frequency [Hz]')\n     plt.xlabel('Time [sec]')\n     plt.show()","metadata":{"id":"Gz_klIXKrfwA","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"88ef3d08-c0eb-413a-b008-1844a677b699"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualization of audio files\n\ndef plot_signals(signals):\n    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n    fig.suptitle('Time Series' , size=16)\n    i=0\n    for x in range(2):\n        for y in range(5):\n            axes[x,y].set_title(list(signals.keys())[i])\n            axes[x,y].plot(list(signals.values())[i])\n            axes[x,y].get_xaxis().set_visible(False)\n            axes[x,y].get_yaxis().set_visible(False)\n            i +=1\n\ndef plot_fft(fft):\n    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n    fig.suptitle('Fourier Transform' , size=16)\n    i=0\n    for x in range(2):\n        for y in range(5):\n            data = list(fft.values())[i]\n            Y,freq = data[0] , data[1]\n            axes[x,y].set_title(list(fft.keys())[i])\n            axes[x,y].plot(freq , Y)\n            axes[x,y].get_xaxis().set_visible(False)\n            axes[x,y].get_yaxis().set_visible(False)\n            i +=1\n\ndef plot_fbank(fbank):\n    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n    fig.suptitle('Filter Bank Coefficients' , size=16)\n    i=0\n    for x in range(2):\n        for y in range(5):\n            axes[x,y].set_title(list(fbank.keys())[i])\n            axes[x,y].imshow(list(fbank.values())[i],cmap='hot', interpolation = 'nearest')\n            axes[x,y].get_xaxis().set_visible(False)\n            axes[x,y].get_yaxis().set_visible(False)\n            i +=1\n\ndef plot_mfccs(mfccs):\n    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n    fig.suptitle('Mel Frequency Capstrum  Coefficients' , size=16)\n    i=0\n    for x in range(2):\n        for y in range(5):\n            axes[x,y].set_title(list(mfccs.keys())[i])\n            axes[x,y].imshow(list(mfccs.values())[i],\n                             cmap='hot', interpolation = 'nearest')\n            axes[x,y].get_xaxis().set_visible(False)\n            axes[x,y].get_yaxis().set_visible(False)\n            i +=1\n\ndef calc_fft(y,rate):\n    n = len(y)\n    freq = np.fft.rfftfreq(n , d= 1/rate)\n    Y= abs(np.fft.rfft(y)/n)\n    return(Y,freq)","metadata":{"id":"aAbD9jBIrfwB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here The Data Set is loaded and plots are Visualised by Calling the Plotting Functions .\nimport matplotlib.pyplot as plt\nfrom scipy.io import wavfile as wav\nfrom scipy.fftpack import fft\nimport numpy as np\nfor file in range(0 , len(listOfFiles) , 1):\n    rate, data = wav.read(listOfFiles[file])\n    fft_out = fft(data)\n    %matplotlib inline\n    plt.plot(data, np.abs(fft_out))\n    plt.show()\n\nsignals={}\nfft={}\nfbank={}\nmfccs={}\n# load data\nfor file in range(0 , len(listOfFiles) , 1):\n#     rate, data = wavfile.read(listOfFiles[file])\n     signal,rate =librosa.load(listOfFiles[file] , sr=44100)\n     mask = envelope(signal , rate , 0.0005)\n     signals[file] = signal\n     fft[file] = calc_fft(signal , rate)\n\n     bank = logfbank(signal[:rate] , rate , nfilt = 26, nfft = 1103).T\n     fbank[file] = bank\n     mel = mfcc(signal[:rate] , rate , numcep =13 , nfilt = 26 , nfft=1103).T\n     mfccs[file]=mel\n\nplot_signals(signals)\nplt.show()\n\nplot_fft(fft)\nplt.show()\n\nplot_fbank(fbank)\nplt.show()\n\nplot_mfccs(mfccs)\nplt.show()","metadata":{"id":"uYo_zhR1rfwB","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"c5c32e21-f6ae-4f5a-f286-d80c8d767d65"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#DOWN SAMPLING OF AUDIO FILES IS DONE  AND PUT MASK OVER IT AND DIRECT INTO CLEAN FOLDER\n#MASK IS TO REMOVE UNNECESSARY EMPTY VOIVES AROUND THE MAIN AUDIO VOICE\ndef envelope(y , rate, threshold):\n    mask=[]\n    y=pd.Series(y).apply(np.abs)\n    y_mean = y.rolling(window=int(rate/10) ,  min_periods=1 , center = True).mean()\n    for mean in y_mean:\n        if mean>threshold:\n            mask.append(True)\n        else:\n            mask.append(False)\n    return mask","metadata":{"id":"xD6YgERcrfwC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The clean Audio Files are redirected to Clean Audio Folder Directory\n\nimport glob\nimport os\nfrom tqdm import tqdm\nimport librosa\nfrom scipy.io import wavfile\n\nfor file in tqdm(glob.glob(r'/kaggle/input/ravdess-emotional-speech-audio/**/*.wav')):\n    try:\n        file_name = os.path.basename(file)\n        signal, rate = librosa.load(file, sr=16000)\n        mask = envelope(signal, rate, 0.0005)  # Assuming you have defined the envelope function\n        wavfile.write(filename=r'/kaggle/input/ravdess-emotional-speech-audio/clean_speech_audio/' + str(file_name), rate=rate, data=signal[mask])\n    except Exception as e:\n        print(f\"Error processing file {file}: {e}\")\n","metadata":{"id":"vkKwPhjnrfwC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Feature Extraction of Audio Files Function\n#Extract features (mfcc, chroma, mel) from a sound file\n\nimport librosa\nimport numpy as np\nimport soundfile as sf\ndef extract_feature(file_name, mfcc, chroma, mel):\n    with sf.SoundFile(file_name) as sound_file:\n        X = sound_file.read(dtype=\"float32\")\n        sample_rate = sound_file.samplerate\n        result = np.array([])\n\n        if mfcc:\n            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n            result = np.hstack((result, mfccs))\n\n        if chroma:\n            stft = np.abs(librosa.stft(X))\n            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n            result = np.hstack((result, chroma))\n\n        if mel:\n            mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n            result = np.hstack((result, mel))\n\n    return result\n","metadata":{"id":"jQj1wi43rfwC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Emotions in the RAVDESS dataset to be classified Audio Files based on .\nemotions={\n  '01':'neutral',\n  '02':'calm',\n  '03':'happy',\n  '04':'sad',\n  '05':'angry',\n  '06':'fearful',\n  '07':'disgust',\n  '08':'surprised'\n}\n#These are the emotions User wants to observe more :\nobserved_emotions=['calm', 'happy', 'fearful', 'disgust']","metadata":{"id":"8bknaZf-rfwC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load the data and extract features for each sound file\nfrom glob import glob\nimport os\nimport glob\ndef load_data(test_size=0.33):\n    x,y=[],[]\n    answer = 0\n    for file in glob.glob(r'/kaggle/input/ravdess-emotional-speech-audio/clean_speech_audio//*.wav'):\n        file_name=os.path.basename(file)\n        emotion=emotions[file_name.split(\"-\")[2]]\n        if emotion not in observed_emotions:\n            answer += 1\n            continue\n        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n        x.append(feature)\n        y.append([emotion,file_name])\n    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)","metadata":{"id":"ll48xzWXrfwC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split the dataset\nimport librosa\nimport numpy as np\nx_train,x_test,y_trai,y_tes=load_data(test_size=0.25)\nprint(np.shape(x_train),np.shape(x_test), np.shape(y_trai),np.shape(y_tes))\ny_test_map = np.array(y_tes).T\ny_test = y_test_map[0]\ntest_filename = y_test_map[1]\ny_train_map = np.array(y_trai).T\ny_train = y_train_map[0]\ntrain_filename = y_train_map[1]\nprint(np.shape(y_train),np.shape(y_test))\nprint(*test_filename,sep=\"\\n\")","metadata":{"id":"Xtu6nquurfwC","outputId":"5a2266ca-bf66-4c9e-8690-eec4e8ff565f","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the shape of the training and testing datasets\n# print((x_train.shape[0], x_test.shape[0]))\nprint((x_train[0], x_test[0]))\n#Get the number of features extracted\nprint(f'Features extracted: {x_train.shape[1]}')","metadata":{"id":"Nk0_e9ttrfwD","outputId":"940f8f10-3198-4c07-aa26-4adb91e7de6d","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the Multi Layer Perceptron Classifier\nmodel=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)","metadata":{"id":"cX-EL2VKrfwD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train the model\nmodel.fit(x_train,y_train)","metadata":{"id":"R12Ia_KwrfwD","outputId":"beea7f29-7511-4858-aaf9-2ea17787c327","colab":{"base_uri":"https://localhost:8080/","height":92}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n# Save the Modle to file in the current working directory\n#For any new testing data other than the data in dataset\n\nPkl_Filename = \"Emotion_Voice_Detection_Model.pkl\"\n\nwith open(Pkl_Filename, 'wb') as file:\n    pickle.dump(model, file)","metadata":{"id":"9wcei4RwrfwD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the Model back from file\nwith open(Pkl_Filename, 'rb') as file:\n    Emotion_Voice_Detection_Model = pickle.load(file)\n\nEmotion_Voice_Detection_Model","metadata":{"id":"zM_TmpETrfwD","outputId":"d40fc68b-efa6-4fef-f9c2-9461974041bf","colab":{"base_uri":"https://localhost:8080/","height":92}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predicting :\ny_pred=Emotion_Voice_Detection_Model.predict(x_test)\ny_pred","metadata":{"id":"nRnJrx0VrfwD","outputId":"f7373487-1a0b-49c5-c970-9a100262e8d5","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Store the Prediction probabilities into CSV file\nimport numpy as np\nimport pandas as pd\ny_pred1 = pd.DataFrame(y_pred, columns=['predictions'])\ny_pred1['file_names'] = test_filename\nprint(y_pred1)\ny_pred1.to_csv('predictionfinal.csv')","metadata":{"id":"_zZo3M6nrfwD","outputId":"63093dbc-eba8-4a26-8106-1826f18034fd","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg","metadata":{"id":"el73n4v43dCG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install PyAudio","metadata":{"id":"IJyUy7m_3hRV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wave\nimport numpy as np\n\n# Define input audio file path\nINPUT_AUDIO_FILE = \"/kaggle/test.wav\"\n\n# Read audio data from the input file\nwith wave.open(INPUT_AUDIO_FILE, 'rb') as wf:\n    num_channels = wf.getnchannels()\n    sample_width = wf.getsampwidth()\n    sample_rate = wf.getframerate()\n    audio_data = wf.readframes(wf.getnframes())\n\n# Define output WAV file path\nWAVE_OUTPUT_FILENAME = \"output10.wav\"\n\n# Write audio data to WAV file\nwith wave.open(WAVE_OUTPUT_FILENAME, 'wb') as wf:\n    wf.setnchannels(num_channels)\n    wf.setsampwidth(sample_width)\n    wf.setframerate(sample_rate)\n    wf.writeframes(audio_data)\n\nprint(f\"Input audio file '{INPUT_AUDIO_FILE}' copied to '{WAVE_OUTPUT_FILENAME}'\")\n","metadata":{"id":"Wgxy37o1rfwD","outputId":"f37f5dbe-f15b-48b0-cddb-a3598b762c80","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install librosa","metadata":{"id":"0tetUGiDrO1v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The file 'output10.wav' in the next cell is the file that was recorded live using the code :\n\nimport librosa\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt  # Import matplotlib.pyplot module\n\ndata, sampling_rate = librosa.load('output10.wav')\n%matplotlib inline\nplt.figure(figsize=(15, 5))\nplt.plot(data)  # Plot the waveform using matplotlib.pyplot.plot()\nplt.xlabel('Time')\nplt.ylabel('Amplitude')\nplt.title('Waveform')\nplt.show()\n","metadata":{"id":"87eECW79rfwD","colab":{"base_uri":"https://localhost:8080/","height":487},"outputId":"99d13f05-a264-4b3a-a63c-ac582efb38b1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Appying extract_feature function on random file and then loading model to predict the result\nfile = 'output10.wav'\nans = []\nnew_feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\nans.append(new_feature)\nans = np.array(ans)\n\n# Assuming Emotion_Voice_Detection_Model.predict expects a numpy array as input\nEmotion_Voice_Detection_Model.predict(ans)","metadata":{"id":"Im2t4ivzrfwE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dbb5462f-0f6b-4399-b0f5-8e3292055734"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"k9awrW6E3qZl"},"execution_count":null,"outputs":[]}]}